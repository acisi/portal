Начнем работу с восстановления старых разработок.
=========================================
В каталоге libs находятся общие библиотеки всех проектов.

Во первых, собиралка прокси-серверов с сайтов скрипты находятся в каталоге scripts, настройки в каталоге settings.
-> Первых имеется скрипт для сборки адресов страниц на которых обычно перечислены адреса открытых проксиков.
   Скрипт в каталоге /scripts/Get_Proxy_Urls.py
   Список ключевых слов для поиска /settings/Proxy_Keywords.data (каждая ключевая фраза с новой строки)

-> В каталог /data сохраняются результаты работы, а во вложенную папку /tmp помещаются промежуточные результаты работы 
(вложенная tmp - это символическая ссылка в /tmp/)

08 Ноября 2013.
= Переписан скрипт для сбора данных с Рамблера (/scripts/Get_Proxy_Urls.py)
= Скрипт сбора данных собсно собирает данные надо дорабатывать большой скрипт который будет грабить страницы и доставать IP-адреса
= _ProxyDaylyScript.sh - Скрип запускаемый ежедневно собирает адреса, скачивает сайты и выбирает из них IP-адреса (Его необходимо добавить в cron для ежедневного запуска)

09 Ноября 2013
= Извлечены старые бэкапы и переписан анализатор IP-адресов на валидность. 
= В текущей версии сбор адресов будет проводится в базу SQLITE

CREATE TABLE "uniq_ip" (
    "id" INTEGER PRIMARY KEY AUTOINCREMENT,
    "ip" TEXT
);
CREATE TABLE sqlite_sequence(name,seq);
CREATE UNIQUE INDEX "u_ip" on uniq_ip (ip ASC);

= Из -за ограничений базы данных sqlite имеет смысл под результаты nmap в дальнейшем выделить другую базу данных. Таким образом разрулим ошибку прошлой версии.
= На завтра надо дописать заполение базы данных и запустить ее в тестирование.

10 ноября 2013 (завершил ProxyDaylyScript)
= Тестируем как будет работать. Надо добавить в cron на автозапуск.
